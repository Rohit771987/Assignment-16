{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a41af5",
   "metadata": {},
   "source": [
    "###  PREDICT THE BURNED AREA OF FOREST FIRES WITH NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64846c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5161fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0  ...         0   \n",
       "\n",
       "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0         0         0         0         1         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         1   \n",
       "3         0         0         0         1         0         0         0   \n",
       "4         0         0         0         1         0         0         0   \n",
       "\n",
       "   monthsep  size_category  \n",
       "0         0          small  \n",
       "1         0          small  \n",
       "2         0          small  \n",
       "3         0          small  \n",
       "4         0          small  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv('forestfires.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285dae7",
   "metadata": {},
   "source": [
    "###  EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaa94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa044f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FFMC   DMC     DC  ISI  temp  RH  wind  rain  area  dayfri  ...  monthfeb  \\\n",
       "0  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0       1  ...         0   \n",
       "1  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0       0  ...         0   \n",
       "2  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0       0  ...         0   \n",
       "3  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0       1  ...         0   \n",
       "4  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0       0  ...         0   \n",
       "\n",
       "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0         0         0         0         1         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         1   \n",
       "3         0         0         0         1         0         0         0   \n",
       "4         0         0         0         1         0         0         0   \n",
       "\n",
       "   monthsep  size_category  \n",
       "0         0          small  \n",
       "1         0          small  \n",
       "2         0          small  \n",
       "3         0          small  \n",
       "4         0          small  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abc4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['size_category']= data1['size_category'].astype('category')\n",
    "data1['size_category'] = data1['size_category'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02454c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  dayfri  ...  \\\n",
       "0    86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00       1  ...   \n",
       "1    90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00       0  ...   \n",
       "2    90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00       0  ...   \n",
       "3    91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00       1  ...   \n",
       "4    89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00       0  ...   \n",
       "..    ...    ...    ...   ...   ...  ..   ...   ...    ...     ...  ...   \n",
       "512  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44       0  ...   \n",
       "513  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29       0  ...   \n",
       "514  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16       0  ...   \n",
       "515  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00       0  ...   \n",
       "516  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00       0  ...   \n",
       "\n",
       "     monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "0           0         0         0         0         1         0         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         1         0         0   \n",
       "4           0         0         0         0         1         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         1   \n",
       "\n",
       "     monthoct  monthsep  size_category  \n",
       "0           0         0              1  \n",
       "1           1         0              1  \n",
       "2           1         0              1  \n",
       "3           0         0              1  \n",
       "4           0         0              1  \n",
       "..        ...       ...            ...  \n",
       "512         0         0              0  \n",
       "513         0         0              0  \n",
       "514         0         0              0  \n",
       "515         0         0              1  \n",
       "516         0         0              1  \n",
       "\n",
       "[517 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43791c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 29)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bcaac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>90.644681</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.332689</td>\n",
       "      <td>0.731141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.520111</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.471632</td>\n",
       "      <td>0.443796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90.200000</td>\n",
       "      <td>68.600000</td>\n",
       "      <td>437.700000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.600000</td>\n",
       "      <td>108.300000</td>\n",
       "      <td>664.200000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.900000</td>\n",
       "      <td>142.400000</td>\n",
       "      <td>713.900000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.200000</td>\n",
       "      <td>291.300000</td>\n",
       "      <td>860.600000</td>\n",
       "      <td>56.100000</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1090.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             FFMC         DMC          DC         ISI        temp          RH  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean    90.644681  110.872340  547.940039    9.021663   18.889168   44.288201   \n",
       "std      5.520111   64.046482  248.066192    4.559477    5.806625   16.317469   \n",
       "min     18.700000    1.100000    7.900000    0.000000    2.200000   15.000000   \n",
       "25%     90.200000   68.600000  437.700000    6.500000   15.500000   33.000000   \n",
       "50%     91.600000  108.300000  664.200000    8.400000   19.300000   42.000000   \n",
       "75%     92.900000  142.400000  713.900000   10.800000   22.800000   53.000000   \n",
       "max     96.200000  291.300000  860.600000   56.100000   33.300000  100.000000   \n",
       "\n",
       "             wind        rain         area      dayfri  ...    monthfeb  \\\n",
       "count  517.000000  517.000000   517.000000  517.000000  ...  517.000000   \n",
       "mean     4.017602    0.021663    12.847292    0.164410  ...    0.038685   \n",
       "std      1.791653    0.295959    63.655818    0.371006  ...    0.193029   \n",
       "min      0.400000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "25%      2.700000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "50%      4.000000    0.000000     0.520000    0.000000  ...    0.000000   \n",
       "75%      4.900000    0.000000     6.570000    0.000000  ...    0.000000   \n",
       "max      9.400000    6.400000  1090.840000    1.000000  ...    1.000000   \n",
       "\n",
       "         monthjan    monthjul    monthjun    monthmar    monthmay    monthnov  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean     0.003868    0.061896    0.032882    0.104449    0.003868    0.001934   \n",
       "std      0.062137    0.241199    0.178500    0.306138    0.062137    0.043980   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         monthoct    monthsep  size_category  \n",
       "count  517.000000  517.000000     517.000000  \n",
       "mean     0.029014    0.332689       0.731141  \n",
       "std      0.168007    0.471632       0.443796  \n",
       "min      0.000000    0.000000       0.000000  \n",
       "25%      0.000000    0.000000       0.000000  \n",
       "50%      0.000000    0.000000       1.000000  \n",
       "75%      0.000000    1.000000       1.000000  \n",
       "max      1.000000    1.000000       1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a04243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 29 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   FFMC           517 non-null    float64\n",
      " 1   DMC            517 non-null    float64\n",
      " 2   DC             517 non-null    float64\n",
      " 3   ISI            517 non-null    float64\n",
      " 4   temp           517 non-null    float64\n",
      " 5   RH             517 non-null    int64  \n",
      " 6   wind           517 non-null    float64\n",
      " 7   rain           517 non-null    float64\n",
      " 8   area           517 non-null    float64\n",
      " 9   dayfri         517 non-null    int64  \n",
      " 10  daymon         517 non-null    int64  \n",
      " 11  daysat         517 non-null    int64  \n",
      " 12  daysun         517 non-null    int64  \n",
      " 13  daythu         517 non-null    int64  \n",
      " 14  daytue         517 non-null    int64  \n",
      " 15  daywed         517 non-null    int64  \n",
      " 16  monthapr       517 non-null    int64  \n",
      " 17  monthaug       517 non-null    int64  \n",
      " 18  monthdec       517 non-null    int64  \n",
      " 19  monthfeb       517 non-null    int64  \n",
      " 20  monthjan       517 non-null    int64  \n",
      " 21  monthjul       517 non-null    int64  \n",
      " 22  monthjun       517 non-null    int64  \n",
      " 23  monthmar       517 non-null    int64  \n",
      " 24  monthmay       517 non-null    int64  \n",
      " 25  monthnov       517 non-null    int64  \n",
      " 26  monthoct       517 non-null    int64  \n",
      " 27  monthsep       517 non-null    int64  \n",
      " 28  size_category  517 non-null    int8   \n",
      "dtypes: float64(8), int64(20), int8(1)\n",
      "memory usage: 113.7 KB\n"
     ]
    }
   ],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b65b79f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FFMC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.431532</td>\n",
       "      <td>-0.300995</td>\n",
       "      <td>-0.028485</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.454771</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.076609</td>\n",
       "      <td>-0.022063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMC</th>\n",
       "      <td>0.382619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>-0.034715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DC</th>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.682192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>0.531857</td>\n",
       "      <td>-0.019428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISI</th>\n",
       "      <td>0.531805</td>\n",
       "      <td>0.305128</td>\n",
       "      <td>0.229154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.068877</td>\n",
       "      <td>0.008726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>0.431532</td>\n",
       "      <td>0.469594</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>0.394287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>-0.006021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>-0.300995</td>\n",
       "      <td>0.073795</td>\n",
       "      <td>-0.039192</td>\n",
       "      <td>-0.132517</td>\n",
       "      <td>-0.527390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.062596</td>\n",
       "      <td>0.045243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>-0.028485</td>\n",
       "      <td>-0.105342</td>\n",
       "      <td>-0.203466</td>\n",
       "      <td>0.106826</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.069410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.181476</td>\n",
       "      <td>-0.059113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.074790</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.099751</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.051733</td>\n",
       "      <td>-0.050001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.040122</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.097844</td>\n",
       "      <td>-0.075519</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>0.056573</td>\n",
       "      <td>-0.311322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayfri</th>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.046695</td>\n",
       "      <td>-0.071949</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.118090</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>-0.052911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>0.107671</td>\n",
       "      <td>0.021810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daymon</th>\n",
       "      <td>-0.059396</td>\n",
       "      <td>-0.107921</td>\n",
       "      <td>-0.052993</td>\n",
       "      <td>-0.158601</td>\n",
       "      <td>-0.136529</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>-0.063881</td>\n",
       "      <td>-0.029945</td>\n",
       "      <td>-0.021206</td>\n",
       "      <td>-0.181293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>0.077125</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>-0.017992</td>\n",
       "      <td>0.060975</td>\n",
       "      <td>0.039632</td>\n",
       "      <td>0.011156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysat</th>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>-0.035189</td>\n",
       "      <td>-0.038585</td>\n",
       "      <td>0.034899</td>\n",
       "      <td>-0.023869</td>\n",
       "      <td>-0.063799</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>-0.195372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>-0.022408</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>-0.019390</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>-0.032783</td>\n",
       "      <td>-0.040397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysun</th>\n",
       "      <td>-0.089517</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.136220</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>-0.017872</td>\n",
       "      <td>-0.020463</td>\n",
       "      <td>-0.210462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>0.050887</td>\n",
       "      <td>-0.018241</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>-0.047726</td>\n",
       "      <td>-0.029568</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>-0.048817</td>\n",
       "      <td>-0.016429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daythu</th>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.087672</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>-0.022406</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>-0.123061</td>\n",
       "      <td>-0.062553</td>\n",
       "      <td>-0.026798</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>-0.162237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042278</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.019300</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.026885</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.016101</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.045985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytue</th>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>-0.014211</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.166728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014491</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.049688</td>\n",
       "      <td>-0.069308</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>0.117121</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>-0.028570</td>\n",
       "      <td>-0.036998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daywed</th>\n",
       "      <td>0.093908</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.090580</td>\n",
       "      <td>-0.087508</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.151487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035713</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.008985</td>\n",
       "      <td>0.043422</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>-0.053222</td>\n",
       "      <td>0.021659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthapr</th>\n",
       "      <td>-0.117199</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.268211</td>\n",
       "      <td>-0.106478</td>\n",
       "      <td>-0.157051</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>0.014001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthaug</th>\n",
       "      <td>0.228103</td>\n",
       "      <td>0.497928</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.334639</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.093101</td>\n",
       "      <td>-0.004187</td>\n",
       "      <td>-0.100837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149116</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.190937</td>\n",
       "      <td>-0.137065</td>\n",
       "      <td>-0.253859</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.032724</td>\n",
       "      <td>-0.128493</td>\n",
       "      <td>-0.524858</td>\n",
       "      <td>0.058954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthdec</th>\n",
       "      <td>-0.137044</td>\n",
       "      <td>-0.176301</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.162322</td>\n",
       "      <td>-0.329648</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>0.269702</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>-0.019140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026701</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.034190</td>\n",
       "      <td>-0.024543</td>\n",
       "      <td>-0.045456</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>-0.186140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthfeb</th>\n",
       "      <td>-0.281535</td>\n",
       "      <td>-0.317899</td>\n",
       "      <td>-0.399277</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.320015</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>-0.014698</td>\n",
       "      <td>-0.020732</td>\n",
       "      <td>0.046323</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.141642</td>\n",
       "      <td>-0.014090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjan</th>\n",
       "      <td>-0.454771</td>\n",
       "      <td>-0.105647</td>\n",
       "      <td>-0.115064</td>\n",
       "      <td>-0.103588</td>\n",
       "      <td>-0.146520</td>\n",
       "      <td>0.170923</td>\n",
       "      <td>-0.070245</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>-0.012589</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>0.037790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjul</th>\n",
       "      <td>0.031833</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>-0.100887</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>-0.013390</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>-0.048969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051528</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.181367</td>\n",
       "      <td>-0.007179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthjun</th>\n",
       "      <td>-0.040634</td>\n",
       "      <td>-0.050403</td>\n",
       "      <td>-0.186183</td>\n",
       "      <td>0.111516</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.012124</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>-0.020314</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036989</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.130195</td>\n",
       "      <td>0.038423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmar</th>\n",
       "      <td>-0.074327</td>\n",
       "      <td>-0.407404</td>\n",
       "      <td>-0.650427</td>\n",
       "      <td>-0.143520</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>-0.089836</td>\n",
       "      <td>0.181433</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068508</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.087722</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.241135</td>\n",
       "      <td>0.035923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthmay</th>\n",
       "      <td>-0.037230</td>\n",
       "      <td>-0.081980</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.060493</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>-0.004566</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012501</td>\n",
       "      <td>-0.003883</td>\n",
       "      <td>-0.016007</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.021282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.032488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthnov</th>\n",
       "      <td>-0.088964</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>-0.078380</td>\n",
       "      <td>-0.076559</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.035885</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>-0.008893</td>\n",
       "      <td>-0.019527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>-0.011308</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.015034</td>\n",
       "      <td>-0.002743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>-0.031083</td>\n",
       "      <td>0.026695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthoct</th>\n",
       "      <td>-0.005998</td>\n",
       "      <td>-0.187632</td>\n",
       "      <td>0.093279</td>\n",
       "      <td>-0.071154</td>\n",
       "      <td>-0.053513</td>\n",
       "      <td>-0.072334</td>\n",
       "      <td>-0.053850</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>-0.045585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.044402</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>-0.059034</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.007610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.122053</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthsep</th>\n",
       "      <td>0.076609</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.531857</td>\n",
       "      <td>-0.068877</td>\n",
       "      <td>0.088006</td>\n",
       "      <td>-0.062596</td>\n",
       "      <td>-0.181476</td>\n",
       "      <td>-0.051733</td>\n",
       "      <td>0.056573</td>\n",
       "      <td>0.107671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141642</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.181367</td>\n",
       "      <td>-0.130195</td>\n",
       "      <td>-0.241135</td>\n",
       "      <td>-0.044001</td>\n",
       "      <td>-0.031083</td>\n",
       "      <td>-0.122053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.044038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_category</th>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.034715</td>\n",
       "      <td>-0.019428</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>0.045243</td>\n",
       "      <td>-0.059113</td>\n",
       "      <td>-0.050001</td>\n",
       "      <td>-0.311322</td>\n",
       "      <td>0.021810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>0.037790</td>\n",
       "      <td>-0.007179</td>\n",
       "      <td>0.038423</td>\n",
       "      <td>0.035923</td>\n",
       "      <td>-0.032488</td>\n",
       "      <td>0.026695</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>-0.044038</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FFMC       DMC        DC       ISI      temp        RH  \\\n",
       "FFMC           1.000000  0.382619  0.330512  0.531805  0.431532 -0.300995   \n",
       "DMC            0.382619  1.000000  0.682192  0.305128  0.469594  0.073795   \n",
       "DC             0.330512  0.682192  1.000000  0.229154  0.496208 -0.039192   \n",
       "ISI            0.531805  0.305128  0.229154  1.000000  0.394287 -0.132517   \n",
       "temp           0.431532  0.469594  0.496208  0.394287  1.000000 -0.527390   \n",
       "RH            -0.300995  0.073795 -0.039192 -0.132517 -0.527390  1.000000   \n",
       "wind          -0.028485 -0.105342 -0.203466  0.106826 -0.227116  0.069410   \n",
       "rain           0.056702  0.074790  0.035861  0.067668  0.069491  0.099751   \n",
       "area           0.040122  0.072994  0.049383  0.008258  0.097844 -0.075519   \n",
       "dayfri         0.019306 -0.012010 -0.004220  0.046695 -0.071949  0.064506   \n",
       "daymon        -0.059396 -0.107921 -0.052993 -0.158601 -0.136529  0.009376   \n",
       "daysat        -0.019637 -0.003653 -0.035189 -0.038585  0.034899 -0.023869   \n",
       "daysun        -0.089517  0.025355 -0.001431 -0.003243  0.014403  0.136220   \n",
       "daythu         0.071730  0.087672  0.051859 -0.022406  0.051432 -0.123061   \n",
       "daytue         0.011225  0.000016  0.028368  0.068610  0.035630 -0.014211   \n",
       "daywed         0.093908  0.017939  0.024803  0.125415  0.090580 -0.087508   \n",
       "monthapr      -0.117199 -0.197543 -0.268211 -0.106478 -0.157051  0.021235   \n",
       "monthaug       0.228103  0.497928  0.279361  0.334639  0.351404  0.054761   \n",
       "monthdec      -0.137044 -0.176301 -0.105642 -0.162322 -0.329648 -0.047714   \n",
       "monthfeb      -0.281535 -0.317899 -0.399277 -0.249777 -0.320015  0.140430   \n",
       "monthjan      -0.454771 -0.105647 -0.115064 -0.103588 -0.146520  0.170923   \n",
       "monthjul       0.031833 -0.001946 -0.100887  0.020982  0.142588  0.013185   \n",
       "monthjun      -0.040634 -0.050403 -0.186183  0.111516  0.051015  0.009382   \n",
       "monthmar      -0.074327 -0.407404 -0.650427 -0.143520 -0.341797 -0.089836   \n",
       "monthmay      -0.037230 -0.081980 -0.114209 -0.060493 -0.045540  0.086822   \n",
       "monthnov      -0.088964 -0.074218 -0.078380 -0.076559 -0.053798 -0.035885   \n",
       "monthoct      -0.005998 -0.187632  0.093279 -0.071154 -0.053513 -0.072334   \n",
       "monthsep       0.076609  0.110907  0.531857 -0.068877  0.088006 -0.062596   \n",
       "size_category -0.022063 -0.034715 -0.019428  0.008726 -0.006021  0.045243   \n",
       "\n",
       "                   wind      rain      area    dayfri  ...  monthfeb  \\\n",
       "FFMC          -0.028485  0.056702  0.040122  0.019306  ... -0.281535   \n",
       "DMC           -0.105342  0.074790  0.072994 -0.012010  ... -0.317899   \n",
       "DC            -0.203466  0.035861  0.049383 -0.004220  ... -0.399277   \n",
       "ISI            0.106826  0.067668  0.008258  0.046695  ... -0.249777   \n",
       "temp          -0.227116  0.069491  0.097844 -0.071949  ... -0.320015   \n",
       "RH             0.069410  0.099751 -0.075519  0.064506  ...  0.140430   \n",
       "wind           1.000000  0.061119  0.012317  0.118090  ... -0.029431   \n",
       "rain           0.061119  1.000000 -0.007366 -0.004261  ... -0.014698   \n",
       "area           0.012317 -0.007366  1.000000 -0.052911  ... -0.020732   \n",
       "dayfri         0.118090 -0.004261 -0.052911  1.000000  ...  0.046323   \n",
       "daymon        -0.063881 -0.029945 -0.021206 -0.181293  ...  0.003933   \n",
       "daysat        -0.063799 -0.032271  0.087868 -0.195372  ...  0.020406   \n",
       "daysun         0.027981 -0.017872 -0.020463 -0.210462  ...  0.008416   \n",
       "daythu        -0.062553 -0.026798  0.020121 -0.162237  ... -0.042278   \n",
       "daytue         0.053396  0.139311 -0.001333 -0.166728  ... -0.014491   \n",
       "daywed        -0.019965 -0.020744 -0.011452 -0.151487  ... -0.035713   \n",
       "monthapr       0.048266 -0.009752 -0.008280 -0.019140  ... -0.026701   \n",
       "monthaug       0.028577  0.093101 -0.004187 -0.100837  ... -0.149116   \n",
       "monthdec       0.269702 -0.009752  0.001010 -0.019140  ... -0.026701   \n",
       "monthfeb      -0.029431 -0.014698 -0.020732  0.046323  ...  1.000000   \n",
       "monthjan      -0.070245 -0.004566 -0.012589 -0.027643  ... -0.012501   \n",
       "monthjul      -0.040645 -0.013390  0.006149 -0.048969  ... -0.051528   \n",
       "monthjun       0.012124 -0.013510 -0.020314  0.006000  ... -0.036989   \n",
       "monthmar       0.181433 -0.020744 -0.045596  0.036205  ... -0.068508   \n",
       "monthmay       0.015054 -0.004566  0.006264  0.056423  ... -0.012501   \n",
       "monthnov       0.011864 -0.003225 -0.008893 -0.019527  ... -0.008831   \n",
       "monthoct      -0.053850 -0.012665 -0.016878 -0.045585  ... -0.034676   \n",
       "monthsep      -0.181476 -0.051733  0.056573  0.107671  ... -0.141642   \n",
       "size_category -0.059113 -0.050001 -0.311322  0.021810  ... -0.014090   \n",
       "\n",
       "               monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "FFMC          -0.454771  0.031833 -0.040634 -0.074327 -0.037230 -0.088964   \n",
       "DMC           -0.105647 -0.001946 -0.050403 -0.407404 -0.081980 -0.074218   \n",
       "DC            -0.115064 -0.100887 -0.186183 -0.650427 -0.114209 -0.078380   \n",
       "ISI           -0.103588  0.020982  0.111516 -0.143520 -0.060493 -0.076559   \n",
       "temp          -0.146520  0.142588  0.051015 -0.341797 -0.045540 -0.053798   \n",
       "RH             0.170923  0.013185  0.009382 -0.089836  0.086822 -0.035885   \n",
       "wind          -0.070245 -0.040645  0.012124  0.181433  0.015054  0.011864   \n",
       "rain          -0.004566 -0.013390 -0.013510 -0.020744 -0.004566 -0.003225   \n",
       "area          -0.012589  0.006149 -0.020314 -0.045596  0.006264 -0.008893   \n",
       "dayfri        -0.027643 -0.048969  0.006000  0.036205  0.056423 -0.019527   \n",
       "daymon        -0.025470 -0.013300  0.017553  0.077125 -0.025470 -0.017992   \n",
       "daysat         0.057019  0.060945 -0.022408  0.021024  0.057019 -0.019390   \n",
       "daysun         0.050887 -0.018241  0.024540 -0.047726 -0.029568 -0.020887   \n",
       "daythu        -0.022793 -0.019300 -0.000195 -0.026885 -0.022793 -0.016101   \n",
       "daytue        -0.023424  0.049688 -0.069308 -0.032351 -0.023424  0.117121   \n",
       "daywed        -0.021282 -0.008985  0.043422 -0.033917 -0.021282 -0.015034   \n",
       "monthapr      -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
       "monthaug      -0.046323 -0.190937 -0.137065 -0.253859 -0.046323 -0.032724   \n",
       "monthdec      -0.008295 -0.034190 -0.024543 -0.045456 -0.008295 -0.005860   \n",
       "monthfeb      -0.012501 -0.051528 -0.036989 -0.068508 -0.012501 -0.008831   \n",
       "monthjan       1.000000 -0.016007 -0.011491 -0.021282 -0.003883 -0.002743   \n",
       "monthjul      -0.016007  1.000000 -0.047363 -0.087722 -0.016007 -0.011308   \n",
       "monthjun      -0.011491 -0.047363  1.000000 -0.062972 -0.011491 -0.008117   \n",
       "monthmar      -0.021282 -0.087722 -0.062972  1.000000 -0.021282 -0.015034   \n",
       "monthmay      -0.003883 -0.016007 -0.011491 -0.021282  1.000000 -0.002743   \n",
       "monthnov      -0.002743 -0.011308 -0.008117 -0.015034 -0.002743  1.000000   \n",
       "monthoct      -0.010772 -0.044402 -0.031874 -0.059034 -0.010772 -0.007610   \n",
       "monthsep      -0.044001 -0.181367 -0.130195 -0.241135 -0.044001 -0.031083   \n",
       "size_category  0.037790 -0.007179  0.038423  0.035923 -0.032488  0.026695   \n",
       "\n",
       "               monthoct  monthsep  size_category  \n",
       "FFMC          -0.005998  0.076609      -0.022063  \n",
       "DMC           -0.187632  0.110907      -0.034715  \n",
       "DC             0.093279  0.531857      -0.019428  \n",
       "ISI           -0.071154 -0.068877       0.008726  \n",
       "temp          -0.053513  0.088006      -0.006021  \n",
       "RH            -0.072334 -0.062596       0.045243  \n",
       "wind          -0.053850 -0.181476      -0.059113  \n",
       "rain          -0.012665 -0.051733      -0.050001  \n",
       "area          -0.016878  0.056573      -0.311322  \n",
       "dayfri        -0.045585  0.107671       0.021810  \n",
       "daymon         0.060975  0.039632       0.011156  \n",
       "daysat         0.017584 -0.032783      -0.040397  \n",
       "daysun         0.007252 -0.048817      -0.016429  \n",
       "daythu        -0.063223  0.008984       0.045985  \n",
       "daytue         0.005008 -0.028570      -0.036998  \n",
       "daywed         0.016325 -0.053222       0.021659  \n",
       "monthapr      -0.023008 -0.093982       0.014001  \n",
       "monthaug      -0.128493 -0.524858       0.058954  \n",
       "monthdec      -0.023008 -0.093982      -0.186140  \n",
       "monthfeb      -0.034676 -0.141642      -0.014090  \n",
       "monthjan      -0.010772 -0.044001       0.037790  \n",
       "monthjul      -0.044402 -0.181367      -0.007179  \n",
       "monthjun      -0.031874 -0.130195       0.038423  \n",
       "monthmar      -0.059034 -0.241135       0.035923  \n",
       "monthmay      -0.010772 -0.044001      -0.032488  \n",
       "monthnov      -0.007610 -0.031083       0.026695  \n",
       "monthoct       1.000000 -0.122053       0.000855  \n",
       "monthsep      -0.122053  1.000000      -0.044038  \n",
       "size_category  0.000855 -0.044038       1.000000  \n",
       "\n",
       "[29 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlation matrics\n",
    "data1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898a6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data converting (array format)\n",
    "data2 = data1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb47607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input X and output Y variable\n",
    "x = data2[:,0:28]\n",
    "y = data2[:,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425589ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 86.2,  26.2,  94.3, ...,   0. ,   0. ,   0. ],\n",
       "       [ 90.6,  35.4, 669.1, ...,   0. ,   1. ,   0. ],\n",
       "       [ 90.6,  43.7, 686.9, ...,   0. ,   1. ,   0. ],\n",
       "       ...,\n",
       "       [ 81.6,  56.7, 665.6, ...,   0. ,   0. ,   0. ],\n",
       "       [ 94.4, 146. , 614.7, ...,   0. ,   0. ,   0. ],\n",
       "       [ 79.5,   3. , 106.7, ...,   1. ,   0. ,   0. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3494232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bb09d",
   "metadata": {},
   "source": [
    "###   Artificial Nural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b8a69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 28, kernel_initializer = 'he_uniform', activation = 'relu'))\n",
    "model.add(Dense(8, kernel_initializer = 'he_uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'he_uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33bb3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c4a766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "18/18 [==============================] - 1s 8ms/step - loss: 1.6276 - accuracy: 0.7601 - val_loss: 1.2104 - val_accuracy: 0.6901\n",
      "Epoch 2/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.7601 - val_loss: 0.6086 - val_accuracy: 0.7076\n",
      "Epoch 3/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7803 - val_loss: 0.5712 - val_accuracy: 0.7076\n",
      "Epoch 4/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7803 - val_loss: 0.5562 - val_accuracy: 0.7076\n",
      "Epoch 5/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7861 - val_loss: 0.5385 - val_accuracy: 0.7076\n",
      "Epoch 6/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7890 - val_loss: 0.5224 - val_accuracy: 0.7193\n",
      "Epoch 7/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4508 - accuracy: 0.7919 - val_loss: 0.5094 - val_accuracy: 0.7251\n",
      "Epoch 8/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7948 - val_loss: 0.4961 - val_accuracy: 0.7251\n",
      "Epoch 9/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.7977 - val_loss: 0.4812 - val_accuracy: 0.7368\n",
      "Epoch 10/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.8092 - val_loss: 0.4654 - val_accuracy: 0.7485\n",
      "Epoch 11/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8266 - val_loss: 0.4288 - val_accuracy: 0.7836\n",
      "Epoch 12/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8584 - val_loss: 0.3541 - val_accuracy: 0.8713\n",
      "Epoch 13/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8671 - val_loss: 0.3924 - val_accuracy: 0.8596\n",
      "Epoch 14/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8757 - val_loss: 0.3139 - val_accuracy: 0.8772\n",
      "Epoch 15/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8699 - val_loss: 0.3266 - val_accuracy: 0.8772\n",
      "Epoch 16/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2861 - accuracy: 0.8931 - val_loss: 0.2903 - val_accuracy: 0.8830\n",
      "Epoch 17/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.8844 - val_loss: 0.3514 - val_accuracy: 0.8889\n",
      "Epoch 18/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.8815 - val_loss: 0.3084 - val_accuracy: 0.8947\n",
      "Epoch 19/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.8902 - val_loss: 0.2635 - val_accuracy: 0.8830\n",
      "Epoch 20/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2479 - accuracy: 0.8902 - val_loss: 0.2525 - val_accuracy: 0.9006\n",
      "Epoch 21/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9017 - val_loss: 0.2444 - val_accuracy: 0.9006\n",
      "Epoch 22/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9017 - val_loss: 0.2364 - val_accuracy: 0.9006\n",
      "Epoch 23/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2228 - accuracy: 0.9104 - val_loss: 0.2319 - val_accuracy: 0.8947\n",
      "Epoch 24/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2185 - accuracy: 0.9162 - val_loss: 0.2287 - val_accuracy: 0.8947\n",
      "Epoch 25/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9220 - val_loss: 0.2271 - val_accuracy: 0.8889\n",
      "Epoch 26/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9191 - val_loss: 0.2088 - val_accuracy: 0.9181\n",
      "Epoch 27/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9306 - val_loss: 0.2320 - val_accuracy: 0.8830\n",
      "Epoch 28/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.9364 - val_loss: 0.2576 - val_accuracy: 0.8713\n",
      "Epoch 29/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9393 - val_loss: 0.1942 - val_accuracy: 0.9415\n",
      "Epoch 30/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1684 - accuracy: 0.9277 - val_loss: 0.1902 - val_accuracy: 0.9474\n",
      "Epoch 31/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.9306 - val_loss: 0.1817 - val_accuracy: 0.9415\n",
      "Epoch 32/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1660 - accuracy: 0.9306 - val_loss: 0.1865 - val_accuracy: 0.9649\n",
      "Epoch 33/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9393 - val_loss: 0.1699 - val_accuracy: 0.9415\n",
      "Epoch 34/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9451 - val_loss: 0.2221 - val_accuracy: 0.8889\n",
      "Epoch 35/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9451 - val_loss: 0.1628 - val_accuracy: 0.9357\n",
      "Epoch 36/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9393 - val_loss: 0.2262 - val_accuracy: 0.8772\n",
      "Epoch 37/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9566 - val_loss: 0.1927 - val_accuracy: 0.9064\n",
      "Epoch 38/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.9480 - val_loss: 0.1537 - val_accuracy: 0.9357\n",
      "Epoch 39/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9480 - val_loss: 0.1508 - val_accuracy: 0.9708\n",
      "Epoch 40/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9653 - val_loss: 0.1470 - val_accuracy: 0.9708\n",
      "Epoch 41/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.9682 - val_loss: 0.1470 - val_accuracy: 0.9357\n",
      "Epoch 42/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9509 - val_loss: 0.1420 - val_accuracy: 0.9357\n",
      "Epoch 43/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9682 - val_loss: 0.1376 - val_accuracy: 0.9883\n",
      "Epoch 44/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9653 - val_loss: 0.1270 - val_accuracy: 0.9532\n",
      "Epoch 45/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9624 - val_loss: 0.1237 - val_accuracy: 0.9591\n",
      "Epoch 46/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1027 - accuracy: 0.9711 - val_loss: 0.1197 - val_accuracy: 0.9591\n",
      "Epoch 47/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.9682 - val_loss: 0.1268 - val_accuracy: 0.9942\n",
      "Epoch 48/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9740 - val_loss: 0.1164 - val_accuracy: 0.9883\n",
      "Epoch 49/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9653 - val_loss: 0.1147 - val_accuracy: 0.9942\n",
      "Epoch 50/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0951 - accuracy: 0.9509 - val_loss: 0.1091 - val_accuracy: 0.9825\n",
      "Epoch 51/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.1114 - accuracy: 0.9595 - val_loss: 0.1060 - val_accuracy: 0.9591\n",
      "Epoch 52/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9682 - val_loss: 0.1042 - val_accuracy: 0.9825\n",
      "Epoch 53/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9798 - val_loss: 0.1261 - val_accuracy: 0.9357\n",
      "Epoch 54/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 0.9682 - val_loss: 0.1341 - val_accuracy: 0.9357\n",
      "Epoch 55/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9711 - val_loss: 0.0972 - val_accuracy: 0.9883\n",
      "Epoch 56/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9798 - val_loss: 0.1141 - val_accuracy: 0.9766\n",
      "Epoch 57/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0708 - accuracy: 0.9827 - val_loss: 0.1470 - val_accuracy: 0.9357\n",
      "Epoch 58/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0914 - val_accuracy: 0.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 0.9769 - val_loss: 0.0889 - val_accuracy: 0.9883\n",
      "Epoch 60/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9740 - val_loss: 0.0879 - val_accuracy: 0.9825\n",
      "Epoch 61/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9798 - val_loss: 0.1027 - val_accuracy: 0.9591\n",
      "Epoch 62/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.9798 - val_loss: 0.1100 - val_accuracy: 0.9532\n",
      "Epoch 63/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9740 - val_loss: 0.0903 - val_accuracy: 0.9883\n",
      "Epoch 64/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9827 - val_loss: 0.1097 - val_accuracy: 0.9532\n",
      "Epoch 65/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9740 - val_loss: 0.0878 - val_accuracy: 0.9883\n",
      "Epoch 66/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9855 - val_loss: 0.1029 - val_accuracy: 0.9532\n",
      "Epoch 67/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9740 - val_loss: 0.1596 - val_accuracy: 0.9298\n",
      "Epoch 68/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9740 - val_loss: 0.0867 - val_accuracy: 0.9591\n",
      "Epoch 69/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9798 - val_loss: 0.0745 - val_accuracy: 0.9883\n",
      "Epoch 70/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9884 - val_loss: 0.0748 - val_accuracy: 0.9766\n",
      "Epoch 71/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9798 - val_loss: 0.0741 - val_accuracy: 0.9883\n",
      "Epoch 72/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9827 - val_loss: 0.0940 - val_accuracy: 0.9766\n",
      "Epoch 73/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9913 - val_loss: 0.0717 - val_accuracy: 0.9883\n",
      "Epoch 74/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9884 - val_loss: 0.0850 - val_accuracy: 0.9825\n",
      "Epoch 75/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9913 - val_loss: 0.0729 - val_accuracy: 0.9766\n",
      "Epoch 76/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9855 - val_loss: 0.0686 - val_accuracy: 0.9942\n",
      "Epoch 77/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9884 - val_loss: 0.0673 - val_accuracy: 0.9883\n",
      "Epoch 78/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9913 - val_loss: 0.0726 - val_accuracy: 0.9883\n",
      "Epoch 79/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 0.9855 - val_loss: 0.0696 - val_accuracy: 0.9766\n",
      "Epoch 80/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9884 - val_loss: 0.1583 - val_accuracy: 0.9298\n",
      "Epoch 81/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9884 - val_loss: 0.1684 - val_accuracy: 0.9123\n",
      "Epoch 82/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9855 - val_loss: 0.0682 - val_accuracy: 0.9825\n",
      "Epoch 83/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9827 - val_loss: 0.0876 - val_accuracy: 0.9649\n",
      "Epoch 84/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9855 - val_loss: 0.0639 - val_accuracy: 0.9942\n",
      "Epoch 85/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 0.9942 - val_loss: 0.0635 - val_accuracy: 0.9883\n",
      "Epoch 86/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9884 - val_loss: 0.0613 - val_accuracy: 0.9942\n",
      "Epoch 87/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9855 - val_loss: 0.0775 - val_accuracy: 0.9708\n",
      "Epoch 88/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.9855 - val_loss: 0.0674 - val_accuracy: 0.9883\n",
      "Epoch 89/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9884 - val_loss: 0.0597 - val_accuracy: 0.9942\n",
      "Epoch 90/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9942 - val_loss: 0.0621 - val_accuracy: 0.9883\n",
      "Epoch 91/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.9971 - val_loss: 0.0785 - val_accuracy: 0.9591\n",
      "Epoch 92/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9769 - val_loss: 0.2302 - val_accuracy: 0.8830\n",
      "Epoch 93/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 0.0718 - val_accuracy: 0.9708\n",
      "Epoch 94/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9884 - val_loss: 0.0580 - val_accuracy: 0.9883\n",
      "Epoch 95/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9913 - val_loss: 0.0852 - val_accuracy: 0.9591\n",
      "Epoch 96/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9884 - val_loss: 0.0569 - val_accuracy: 0.9942\n",
      "Epoch 97/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.0881 - val_accuracy: 0.9591\n",
      "Epoch 98/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9913 - val_loss: 0.0591 - val_accuracy: 0.9883\n",
      "Epoch 99/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9884 - val_loss: 0.0862 - val_accuracy: 0.9591\n",
      "Epoch 100/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.9913 - val_loss: 0.0570 - val_accuracy: 0.9883\n",
      "Epoch 101/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9884 - val_loss: 0.0590 - val_accuracy: 0.9883\n",
      "Epoch 102/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9884 - val_loss: 0.0579 - val_accuracy: 0.9825\n",
      "Epoch 103/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.9884 - val_loss: 0.0541 - val_accuracy: 0.9942\n",
      "Epoch 104/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9971 - val_loss: 0.0656 - val_accuracy: 0.9766\n",
      "Epoch 105/150\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9827 - val_loss: 0.0757 - val_accuracy: 0.9532\n",
      "Epoch 106/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9855 - val_loss: 0.0547 - val_accuracy: 0.9942\n",
      "Epoch 107/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.9942 - val_loss: 0.0538 - val_accuracy: 0.9825\n",
      "Epoch 108/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9884 - val_loss: 0.0610 - val_accuracy: 0.9766\n",
      "Epoch 109/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9913 - val_loss: 0.0578 - val_accuracy: 0.9825\n",
      "Epoch 110/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9942 - val_loss: 0.0556 - val_accuracy: 0.9825\n",
      "Epoch 111/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9855 - val_loss: 0.0754 - val_accuracy: 0.9591\n",
      "Epoch 112/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9798 - val_loss: 0.1003 - val_accuracy: 0.9532\n",
      "Epoch 113/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9740 - val_loss: 0.0545 - val_accuracy: 0.9825\n",
      "Epoch 114/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9855 - val_loss: 0.0755 - val_accuracy: 0.9591\n",
      "Epoch 115/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 0.0574 - val_accuracy: 0.9825\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9884 - val_loss: 0.0555 - val_accuracy: 0.9825\n",
      "Epoch 117/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9913 - val_loss: 0.0757 - val_accuracy: 0.9649\n",
      "Epoch 118/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9827 - val_loss: 0.0556 - val_accuracy: 0.9825\n",
      "Epoch 119/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.9942 - val_loss: 0.1034 - val_accuracy: 0.9532\n",
      "Epoch 120/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9913 - val_loss: 0.0522 - val_accuracy: 0.9883\n",
      "Epoch 121/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9913 - val_loss: 0.0750 - val_accuracy: 0.9591\n",
      "Epoch 122/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 0.9913 - val_loss: 0.0546 - val_accuracy: 0.9825\n",
      "Epoch 123/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9884 - val_loss: 0.1011 - val_accuracy: 0.9474\n",
      "Epoch 124/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9827 - val_loss: 0.0544 - val_accuracy: 0.9825\n",
      "Epoch 125/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9769 - val_loss: 0.1219 - val_accuracy: 0.9474\n",
      "Epoch 126/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9855 - val_loss: 0.0662 - val_accuracy: 0.9649\n",
      "Epoch 127/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9769 - val_loss: 0.0549 - val_accuracy: 0.9766\n",
      "Epoch 128/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9971 - val_loss: 0.0837 - val_accuracy: 0.9532\n",
      "Epoch 129/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0310 - accuracy: 0.9913 - val_loss: 0.0640 - val_accuracy: 0.9649\n",
      "Epoch 130/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9942 - val_loss: 0.0508 - val_accuracy: 0.9883\n",
      "Epoch 131/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9913 - val_loss: 0.0592 - val_accuracy: 0.9766\n",
      "Epoch 132/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.0466 - val_accuracy: 0.9883\n",
      "Epoch 133/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9971 - val_loss: 0.0474 - val_accuracy: 0.9883\n",
      "Epoch 134/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9884 - val_loss: 0.0505 - val_accuracy: 0.9825\n",
      "Epoch 135/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.0528 - val_accuracy: 0.9825\n",
      "Epoch 136/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9827 - val_loss: 0.0502 - val_accuracy: 0.9883\n",
      "Epoch 137/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9942 - val_loss: 0.0565 - val_accuracy: 0.9708\n",
      "Epoch 138/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9884 - val_loss: 0.0500 - val_accuracy: 0.9825\n",
      "Epoch 139/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9855 - val_loss: 0.0616 - val_accuracy: 0.9649\n",
      "Epoch 140/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9766\n",
      "Epoch 141/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9913 - val_loss: 0.0494 - val_accuracy: 0.9825\n",
      "Epoch 142/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9942 - val_loss: 0.0481 - val_accuracy: 0.9825\n",
      "Epoch 143/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 0.0509 - val_accuracy: 0.9825\n",
      "Epoch 144/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9971 - val_loss: 0.0635 - val_accuracy: 0.9708\n",
      "Epoch 145/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9884 - val_loss: 0.0847 - val_accuracy: 0.9591\n",
      "Epoch 146/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9971 - val_loss: 0.0492 - val_accuracy: 0.9883\n",
      "Epoch 147/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9766\n",
      "Epoch 148/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9971 - val_loss: 0.0495 - val_accuracy: 0.9825\n",
      "Epoch 149/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9942 - val_loss: 0.0751 - val_accuracy: 0.9591\n",
      "Epoch 150/150\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9884 - val_loss: 0.0511 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a61b297c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(x,y, validation_split = 0.33, epochs= 150, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d6e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 623us/step - loss: 0.0291 - accuracy: 0.9903\n",
      "accuracy\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "score = model.evaluate(x,y)\n",
    "print((model.metrics_names[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119ff57",
   "metadata": {},
   "source": [
    "###  Artificial Neural Network Model - Tuning of All Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df70bc7",
   "metadata": {},
   "source": [
    "#### Tuning of Hyperparameter : Batch size, epochs, learning rate, Dropout,  Activation, Kernel Initializer, Number of neurons in activation number(nueron1 and nueron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ff6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2cbaaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.754024e-15</td>\n",
       "      <td>3.070830e-16</td>\n",
       "      <td>7.387171e-17</td>\n",
       "      <td>-3.865380e-17</td>\n",
       "      <td>2.005703e-16</td>\n",
       "      <td>3.362881e-16</td>\n",
       "      <td>-2.676776e-16</td>\n",
       "      <td>-2.841054e-16</td>\n",
       "      <td>-1.274502e-16</td>\n",
       "      <td>4.874674e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.179943e-16</td>\n",
       "      <td>-1.933764e-16</td>\n",
       "      <td>-2.260174e-17</td>\n",
       "      <td>1.352883e-17</td>\n",
       "      <td>1.169277e-16</td>\n",
       "      <td>2.265542e-16</td>\n",
       "      <td>-2.596515e-16</td>\n",
       "      <td>1.443075e-16</td>\n",
       "      <td>6.253326e-16</td>\n",
       "      <td>4.024290e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.304582e+01</td>\n",
       "      <td>-1.715608e+00</td>\n",
       "      <td>-2.179108e+00</td>\n",
       "      <td>-1.980578e+00</td>\n",
       "      <td>-2.876943e+00</td>\n",
       "      <td>-1.796637e+00</td>\n",
       "      <td>-2.021098e+00</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-2.020198e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.063453e-02</td>\n",
       "      <td>-6.606652e-01</td>\n",
       "      <td>-4.448281e-01</td>\n",
       "      <td>-5.535954e-01</td>\n",
       "      <td>-5.842379e-01</td>\n",
       "      <td>-6.924563e-01</td>\n",
       "      <td>-7.361236e-01</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-2.020198e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.732292e-01</td>\n",
       "      <td>-4.020255e-02</td>\n",
       "      <td>4.691190e-01</td>\n",
       "      <td>-1.364774e-01</td>\n",
       "      <td>7.082076e-02</td>\n",
       "      <td>-1.403660e-01</td>\n",
       "      <td>-9.833712e-03</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-1.938429e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.089598e-01</td>\n",
       "      <td>4.927389e-01</td>\n",
       "      <td>6.696628e-01</td>\n",
       "      <td>3.904086e-01</td>\n",
       "      <td>6.741643e-01</td>\n",
       "      <td>5.344111e-01</td>\n",
       "      <td>4.929823e-01</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-9.870852e-02</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>1.416268e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.007353e+00</td>\n",
       "      <td>2.819865e+00</td>\n",
       "      <td>1.261610e+00</td>\n",
       "      <td>1.033538e+01</td>\n",
       "      <td>2.484195e+00</td>\n",
       "      <td>3.417549e+00</td>\n",
       "      <td>3.007063e+00</td>\n",
       "      <td>2.157228e+01</td>\n",
       "      <td>1.695111e+01</td>\n",
       "      <td>2.254407e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.512952e+00</td>\n",
       "      <td>4.984977e+00</td>\n",
       "      <td>1.604681e+01</td>\n",
       "      <td>3.893103e+00</td>\n",
       "      <td>5.423261e+00</td>\n",
       "      <td>2.928152e+00</td>\n",
       "      <td>1.604681e+01</td>\n",
       "      <td>2.271563e+01</td>\n",
       "      <td>5.785038e+00</td>\n",
       "      <td>1.416268e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean  -1.754024e-15  3.070830e-16  7.387171e-17 -3.865380e-17  2.005703e-16   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.304582e+01 -1.715608e+00 -2.179108e+00 -1.980578e+00 -2.876943e+00   \n",
       "25%   -8.063453e-02 -6.606652e-01 -4.448281e-01 -5.535954e-01 -5.842379e-01   \n",
       "50%    1.732292e-01 -4.020255e-02  4.691190e-01 -1.364774e-01  7.082076e-02   \n",
       "75%    4.089598e-01  4.927389e-01  6.696628e-01  3.904086e-01  6.741643e-01   \n",
       "max    1.007353e+00  2.819865e+00  1.261610e+00  1.033538e+01  2.484195e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   3.362881e-16 -2.676776e-16 -2.841054e-16 -1.274502e-16  4.874674e-17   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.796637e+00 -2.021098e+00 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
       "25%   -6.924563e-01 -7.361236e-01 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
       "50%   -1.403660e-01 -9.833712e-03 -7.326831e-02 -1.938429e-01 -4.435755e-01   \n",
       "75%    5.344111e-01  4.929823e-01 -7.326831e-02 -9.870852e-02 -4.435755e-01   \n",
       "max    3.417549e+00  3.007063e+00  2.157228e+01  1.695111e+01  2.254407e+00   \n",
       "\n",
       "       ...            18            19            20            21  \\\n",
       "count  ...  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   ...  7.179943e-16 -1.933764e-16 -2.260174e-17  1.352883e-17   \n",
       "std    ...  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "25%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "50%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "75%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "max    ...  7.512952e+00  4.984977e+00  1.604681e+01  3.893103e+00   \n",
       "\n",
       "                 22            23            24            25            26  \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   1.169277e-16  2.265542e-16 -2.596515e-16  1.443075e-16  6.253326e-16   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "25%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "50%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "75%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "max    5.423261e+00  2.928152e+00  1.604681e+01  2.271563e+01  5.785038e+00   \n",
       "\n",
       "                 27  \n",
       "count  5.170000e+02  \n",
       "mean   4.024290e-16  \n",
       "std    1.000969e+00  \n",
       "min   -7.060812e-01  \n",
       "25%   -7.060812e-01  \n",
       "50%   -7.060812e-01  \n",
       "75%    1.416268e+00  \n",
       "max    1.416268e+00  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(x)\n",
    "x_std = sc.transform(x)\n",
    "pd.DataFrame(x_std).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f26b7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model1(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1, input_dim = 28, kernel_initializer = init,activation= activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2, input_dim = neuron1, activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation ='sigmoid'))\n",
    "    \n",
    "    adam = adam_v2.Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss ='binary_crossentropy', optimizer =  adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a3b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model1 = KerasClassifier(build_fn = create_model1, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "405a6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid serch parameter\n",
    "batch_size = [10,20]\n",
    "epochs = [10,50]\n",
    "learning_rate = [0.01,0.1]\n",
    "dropout_rate = [0.1,0.2]\n",
    "activation_function = ['softmax','tanh']\n",
    "init = ['uniform','normal']\n",
    "neuron1 = [4,8,]\n",
    "neuron2 = [2,4,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08788b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dictionary of the grid search parameter\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "256680f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "[CV 1/5; 1/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 1/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 1/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 1/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 1/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 1/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 1/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 1/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 1/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 1/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 2/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 2/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 2/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 2/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 2/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 2/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 2/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 2/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 2/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 2/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 3/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 3/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 3/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 3/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 3/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 3/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 3/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 3/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 3/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 3/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 1/5; 4/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 4/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 4/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 4/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 4/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 4/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 4/5; 4/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 4/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 4/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 4/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 5/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 5/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 2/5; 5/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 5/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 5/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 5/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 5/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 5/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 5/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 5/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 6/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 6/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 6/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 6/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 6/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 6/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 6/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 6/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 6/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 6/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 7/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 7/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 7/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 7/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 7/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 7/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 7/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 7/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 5/5; 7/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 7/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 8/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 8/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 8/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 8/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 3/5; 8/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 8/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 8/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 8/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 8/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 8/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 1/5; 9/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 9/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 9/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 9/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 9/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 9/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 9/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 9/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 9/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 9/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 10/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 10/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 2/5; 10/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 10/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 10/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 10/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 10/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 10/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 5/5; 10/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 10/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 11/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 11/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 11/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 11/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 11/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 11/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 11/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 11/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 11/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 11/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 12/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 12/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 12/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 12/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 12/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 12/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 12/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 12/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 12/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 12/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 13/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 13/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 13/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 13/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 13/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 13/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 13/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 13/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 5/5; 13/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 13/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 1/5; 14/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 14/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 2/5; 14/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 14/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 14/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 14/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 14/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 14/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 14/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 14/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 15/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 15/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 15/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 15/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 15/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 15/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 4/5; 15/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 15/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 15/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 15/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 16/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 16/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 16/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 16/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 16/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 16/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 16/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 16/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 5/5; 16/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 16/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 17/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 17/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 2/5; 17/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 17/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 17/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 17/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 17/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 17/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.4s\n",
      "[CV 5/5; 17/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 17/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 18/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 18/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 18/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 18/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 3/5; 18/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 18/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 4/5; 18/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 18/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 18/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 18/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 19/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 19/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 19/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 19/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 19/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 19/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 19/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 19/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 19/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 19/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 1/5; 20/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 20/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 20/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 20/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 20/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 20/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 20/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 20/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 20/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 20/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.3s\n",
      "[CV 1/5; 21/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 21/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 21/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 21/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 21/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 21/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 21/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 21/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 21/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 21/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 22/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 22/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.3s\n",
      "[CV 2/5; 22/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 22/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 3/5; 22/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 22/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 22/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 22/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 22/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 22/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 23/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 23/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 2/5; 23/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 23/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 3/5; 23/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 23/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 23/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 23/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 5/5; 23/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 23/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 1/5; 24/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 24/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 24/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 24/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 3/5; 24/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 24/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 24/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 24/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 24/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 24/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 25/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 25/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 25/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 25/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 3/5; 25/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 25/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 4/5; 25/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 25/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 5/5; 25/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 25/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 26/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 26/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 26/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 26/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 3/5; 26/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 26/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.4s\n",
      "[CV 4/5; 26/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 26/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 26/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 26/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 27/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 27/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 2/5; 27/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 27/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 27/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 27/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 27/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 27/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 27/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 27/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 1/5; 28/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 28/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 28/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 28/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.3s\n",
      "[CV 3/5; 28/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 28/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 28/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 28/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 5/5; 28/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 28/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 29/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 29/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 2/5; 29/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 29/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 29/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 29/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.3s\n",
      "[CV 4/5; 29/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 29/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 29/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 29/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 30/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 30/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 30/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 30/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 30/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 30/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 4/5; 30/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 30/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 30/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 30/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 31/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 31/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 2/5; 31/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 31/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 31/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 31/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 4/5; 31/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 31/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 31/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 31/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 1/5; 32/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 32/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 32/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 32/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 32/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 32/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 32/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 32/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.3s\n",
      "[CV 5/5; 32/256] START activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 32/256] END activation_function=softmax, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 33/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 33/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 33/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 33/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 33/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 33/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 33/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 33/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 33/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 33/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 34/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 34/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 34/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 34/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 34/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 34/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 34/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 34/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 34/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 34/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 35/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 35/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 35/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 35/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 35/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 35/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 35/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 35/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 35/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 35/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 1/5; 36/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 36/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 36/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 36/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 36/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 36/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 36/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 36/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 36/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 36/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 1/5; 37/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 37/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 37/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 37/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 37/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 37/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 37/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 37/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 37/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 37/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 38/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 38/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 38/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 38/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 38/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 38/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 38/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 38/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 38/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 38/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 1/5; 39/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 39/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 2/5; 39/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 39/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 39/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 39/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 39/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 39/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 39/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 39/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 40/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 40/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 40/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 40/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 40/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 40/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 4/5; 40/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 40/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 40/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 40/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 41/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 41/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 41/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 41/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 41/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 41/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 41/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 41/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 41/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 41/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 42/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 42/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 42/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 42/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 42/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 42/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 42/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 42/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 42/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 42/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 1/5; 43/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 43/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 43/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 43/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 43/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 43/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 43/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 43/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 43/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 43/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 1/5; 44/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 44/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 44/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 44/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 44/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 44/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 44/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 44/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 44/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 44/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 45/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 45/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 45/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 45/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 45/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 45/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 4/5; 45/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 45/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 45/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 45/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 46/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 46/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 46/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 46/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 46/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 46/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 46/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 46/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 46/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 46/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 47/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 47/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 47/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 47/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 47/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 47/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 47/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 47/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 47/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 47/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 48/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 48/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 48/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 48/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 48/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 48/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 48/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 48/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 5/5; 48/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 48/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 49/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 49/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 2/5; 49/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 49/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 49/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 49/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 49/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 49/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 49/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 49/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 50/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 50/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.3s\n",
      "[CV 2/5; 50/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 50/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 50/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 50/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 4/5; 50/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 50/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 50/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 50/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 51/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 51/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 51/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 51/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 3/5; 51/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 51/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 4/5; 51/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 51/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.4s\n",
      "[CV 5/5; 51/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 51/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 52/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 52/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 52/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 52/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 3/5; 52/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 52/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 4/5; 52/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 52/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 52/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 52/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 53/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 53/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 53/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 53/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 53/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 53/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.4s\n",
      "[CV 4/5; 53/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 53/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 5/5; 53/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 53/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 54/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 54/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 54/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 54/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 54/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 54/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 54/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 54/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 54/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 54/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 55/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 55/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 2/5; 55/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 55/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 55/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 55/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 55/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 55/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 55/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 55/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 56/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 56/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 2/5; 56/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 56/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 3/5; 56/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 56/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 56/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 56/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 56/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 56/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 57/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 57/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 2/5; 57/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 57/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 57/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 57/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 57/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 57/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 5/5; 57/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 57/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 58/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 58/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 58/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 58/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 58/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 58/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 58/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 58/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 5/5; 58/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 58/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 59/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 59/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 59/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 59/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 3/5; 59/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 59/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 4/5; 59/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 59/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 59/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 59/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 60/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 60/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 60/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 60/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 60/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 60/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 60/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 60/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 60/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 60/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 61/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 61/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 2/5; 61/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 61/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 3/5; 61/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 61/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 61/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 61/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 61/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 61/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 62/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 62/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 62/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 62/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 62/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 62/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 62/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 62/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 5/5; 62/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 62/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 63/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 63/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 2/5; 63/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 63/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 63/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 63/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 63/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 63/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 63/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 63/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 1/5; 64/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 64/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 64/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 64/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 64/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 64/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 64/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 64/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 64/256] START activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 64/256] END activation_function=softmax, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 65/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 65/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 65/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 65/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 65/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 65/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 65/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 65/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 65/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 65/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 1/5; 66/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 66/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 66/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 66/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 66/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 66/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 66/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 66/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 66/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 66/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 67/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 67/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 67/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 67/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 67/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 67/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 4/5; 67/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 67/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 67/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 67/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 68/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 68/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 68/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 68/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 68/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 68/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 68/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 68/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 68/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 68/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 69/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 69/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 69/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 69/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 69/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 69/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 69/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 69/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 69/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 69/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 70/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 70/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 70/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 70/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 70/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 70/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 70/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 70/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 70/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 70/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 71/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 71/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 71/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 71/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 71/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 71/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 71/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 71/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 71/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 71/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 72/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 72/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 2/5; 72/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 72/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 72/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 72/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 72/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 72/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 72/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 72/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 73/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 73/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 73/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 73/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 73/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 73/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 73/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 73/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 73/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 73/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 74/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 74/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 74/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 74/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 74/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 74/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 74/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 74/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 74/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 74/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 75/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 75/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 75/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 75/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 75/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 75/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 4/5; 75/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 75/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 75/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 75/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 76/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 76/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 2/5; 76/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 76/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 76/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 76/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 76/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 76/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 76/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 76/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 77/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 77/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 77/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 77/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 77/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 77/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 77/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 77/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 77/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 77/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 78/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 78/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 2/5; 78/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 78/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 3/5; 78/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 78/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 78/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 78/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 78/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 78/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 79/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 79/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 79/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 79/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 79/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 79/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 79/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 79/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 79/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 79/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 80/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 80/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 2/5; 80/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 80/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 80/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 80/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 80/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 80/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 80/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 80/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 1/5; 81/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 81/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 81/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 81/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 81/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 81/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 81/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 81/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 81/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 81/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 82/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 82/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 82/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 82/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 82/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 82/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 82/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 82/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 82/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 82/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 83/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 83/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 83/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 83/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 3/5; 83/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 83/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 83/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 83/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 83/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 83/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 84/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 84/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 84/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 84/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 84/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 84/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 84/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 84/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 84/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 84/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 1/5; 85/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 85/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 85/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 85/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 85/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 85/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 85/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 85/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 85/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 85/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 86/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 86/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 86/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 86/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 86/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 86/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 86/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 86/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 86/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 86/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 87/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 87/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 87/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 87/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 87/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 87/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 87/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 87/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 87/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 87/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 88/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 88/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 88/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 88/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 88/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 88/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 88/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 88/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 88/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 88/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 89/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 89/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 89/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 89/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 89/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 89/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 89/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 89/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 89/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 89/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 90/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 90/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 90/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 90/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 90/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 90/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 4/5; 90/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 90/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 90/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 90/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 91/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 91/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 91/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 91/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 91/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 91/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 4/5; 91/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 91/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 91/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 91/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 92/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 92/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 92/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 92/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 92/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 92/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 92/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 92/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 92/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 92/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 93/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 93/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 93/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 93/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 93/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 93/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 93/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 93/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 93/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 93/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 94/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 94/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 94/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 94/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 94/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 94/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 94/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 94/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 94/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 94/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 1/5; 95/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 95/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 95/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 95/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 95/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 95/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 95/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 95/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 95/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 95/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 96/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 96/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 2/5; 96/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 96/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 96/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 96/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 96/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 96/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 96/256] START activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 96/256] END activation_function=softmax, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 97/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 97/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 97/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 97/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 97/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 97/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 97/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 97/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 97/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 97/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 98/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 98/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 98/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 98/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 98/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 98/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 98/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 98/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 98/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 98/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 99/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 99/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 99/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 99/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 99/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 99/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 99/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 99/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 99/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 99/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 1/5; 100/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 100/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 100/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 100/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 100/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 100/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 100/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 100/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 100/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 100/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 101/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 101/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 101/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 101/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 101/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 101/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 101/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 101/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 101/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 101/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 102/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 102/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 102/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 102/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 102/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 102/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 102/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 102/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 102/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 102/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 103/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 103/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 103/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 103/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 103/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 103/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 103/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 103/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 5/5; 103/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 103/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 104/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 104/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 104/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 104/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 104/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 104/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 4/5; 104/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 104/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 104/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 104/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 105/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 105/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 105/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 105/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 105/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 105/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 105/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 105/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 105/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 105/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 1/5; 106/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 106/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 106/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 106/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 106/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 106/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 106/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 106/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 106/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 106/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 107/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 107/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 107/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 107/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 107/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 107/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 4/5; 107/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 107/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 107/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 107/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 1/5; 108/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 108/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 2/5; 108/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 108/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 108/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 108/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 108/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 108/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 108/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 108/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 109/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 109/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 109/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 109/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 109/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 109/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 109/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 109/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 109/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 109/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 1/5; 110/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 110/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 110/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 110/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 110/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 110/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 110/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 110/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 110/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 110/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 111/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 111/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 111/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 111/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 111/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 111/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 4/5; 111/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 111/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 111/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 111/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 112/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 112/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 112/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 112/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 112/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 112/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 112/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 112/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 112/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 112/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 113/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 113/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 2/5; 113/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 113/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 113/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 113/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 113/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 113/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 113/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 113/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 114/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 114/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 114/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 114/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 114/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 114/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 114/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 114/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 114/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 114/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 115/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 115/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 2/5; 115/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 115/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 115/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 115/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 115/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 115/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 115/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 115/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 116/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 116/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 116/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 116/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 116/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 116/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 116/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 116/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 116/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 116/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 117/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 117/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 117/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 117/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 117/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 117/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 117/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 117/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 117/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 117/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 1/5; 118/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 118/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 118/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 118/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 118/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 118/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 118/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 118/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 118/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 118/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 119/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 119/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 119/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 119/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 119/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 119/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 119/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 119/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 119/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 119/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 120/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 120/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 120/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 120/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 3/5; 120/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 120/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 120/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 120/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 120/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 120/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 1/5; 121/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 121/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 121/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 121/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 121/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 121/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 4/5; 121/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 121/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 121/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 121/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 1/5; 122/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 122/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 122/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 122/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 122/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 122/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 122/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 122/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 122/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 122/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 123/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 123/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 123/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 123/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 123/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 123/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 123/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 123/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 123/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 123/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 124/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 124/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 124/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 124/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 124/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 124/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 124/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 124/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 124/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 124/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 125/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 125/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 125/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 125/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 125/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 125/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 125/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 125/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 5/5; 125/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 125/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 1/5; 126/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 126/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 2/5; 126/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 126/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 126/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 126/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 126/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 126/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 126/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 126/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.0s\n",
      "[CV 1/5; 127/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 127/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 127/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 127/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 127/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 127/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 127/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 127/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 5/5; 127/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 127/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 128/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 128/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 2/5; 128/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 128/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 128/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 128/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 128/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 128/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 128/256] START activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 128/256] END activation_function=softmax, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 1/5; 129/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 129/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 129/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 129/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 129/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 129/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 129/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 129/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 129/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 129/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 1/5; 130/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 130/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 130/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 130/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 3/5; 130/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 130/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 4/5; 130/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 130/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 130/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 130/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 131/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 131/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 131/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 131/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 131/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 131/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 131/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 131/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 131/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 131/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 132/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 132/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 132/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 132/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 132/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 132/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 132/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 132/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 132/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 132/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 133/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 133/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 133/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 133/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 133/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 133/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 133/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 133/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 133/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 133/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 134/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 134/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 134/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 134/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 3/5; 134/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 134/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 134/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 134/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 134/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 134/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 135/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 135/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 135/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 135/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 135/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 135/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 135/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 135/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 135/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 135/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 136/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 136/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 136/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 136/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 136/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 136/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 136/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 136/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 136/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 136/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 137/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 137/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 137/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 137/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 3/5; 137/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 137/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 137/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 137/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 137/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 137/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 138/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 138/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 2/5; 138/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 138/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 138/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 138/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.7s\n",
      "[CV 4/5; 138/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 138/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 138/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 138/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 139/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 139/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 139/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 139/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 139/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 139/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 139/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 139/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 5/5; 139/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 139/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 140/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 140/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 140/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 140/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 140/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 140/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 140/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 140/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 140/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 140/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 141/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 141/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 2/5; 141/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 141/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 3/5; 141/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 141/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 141/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 141/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 5/5; 141/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 141/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 142/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 142/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 142/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 142/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 142/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 142/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 142/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 142/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 142/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 142/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 1/5; 143/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 143/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 143/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 143/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 143/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 143/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 143/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 143/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 143/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 143/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 144/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 144/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 144/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 144/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 144/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 144/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 144/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 144/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 144/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 144/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 145/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 145/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 145/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 145/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 145/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 145/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 145/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 145/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 145/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 145/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 146/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 146/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 146/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 146/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 146/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 146/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 146/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 146/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 146/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 146/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 147/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 147/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 147/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 147/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 147/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 147/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 4/5; 147/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 147/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 147/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 147/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 148/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 148/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.3s\n",
      "[CV 2/5; 148/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 148/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 148/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 148/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 148/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 148/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 148/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 148/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 1/5; 149/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 149/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 149/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 149/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 149/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 149/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 4/5; 149/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 149/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 149/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 149/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 150/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 150/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 150/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 150/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.3s\n",
      "[CV 3/5; 150/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 150/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 150/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 150/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 150/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 150/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 151/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 151/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 2/5; 151/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 151/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 151/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 151/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 151/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 151/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 151/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 151/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 152/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 152/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 152/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 152/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 152/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 152/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 152/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 152/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 152/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 152/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.5s\n",
      "[CV 1/5; 153/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 153/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.4s\n",
      "[CV 2/5; 153/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 153/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 153/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 153/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 153/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 153/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 153/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 153/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 1/5; 154/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 154/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 154/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 154/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 154/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 154/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 154/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 154/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 154/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 154/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 155/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 155/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 155/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 155/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 155/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 155/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 155/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 155/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 155/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 155/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 156/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 156/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 2/5; 156/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 156/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 156/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 156/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 156/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 156/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 156/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 156/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 157/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 157/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 157/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 157/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 157/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 157/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 4/5; 157/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 157/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 5/5; 157/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 157/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 1/5; 158/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 158/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 158/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 158/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 158/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 158/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 158/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 158/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 158/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 158/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 159/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 159/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 159/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 159/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 159/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 159/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 159/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 159/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 159/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 159/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 160/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 160/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 160/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 160/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 160/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 160/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 160/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 160/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 160/256] START activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 160/256] END activation_function=tanh, batch_size=10, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 161/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 161/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 161/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 161/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 161/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 161/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 161/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 161/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 5/5; 161/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 161/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 162/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 162/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 162/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 162/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 162/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 162/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 162/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 162/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 162/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 162/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 163/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 163/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 163/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 163/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 163/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 163/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 163/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 163/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 163/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 163/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 164/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 164/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 164/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 164/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 164/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 164/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 164/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 164/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 164/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 164/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 165/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 165/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 165/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 165/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 165/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 165/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 165/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 165/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 165/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 165/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 1/5; 166/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 166/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 166/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 166/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 166/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 166/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 166/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 166/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 166/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 166/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 167/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 167/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 167/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 167/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 167/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 167/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 167/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 167/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 167/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 167/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 168/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 168/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 168/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 168/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 168/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 168/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 168/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 168/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 168/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 168/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 169/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 169/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 169/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 169/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 169/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 169/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 169/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 169/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 169/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 169/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 170/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 170/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 170/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 170/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 170/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 170/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 170/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 170/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 170/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 170/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 171/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 171/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 171/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 171/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 171/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 171/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 4/5; 171/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 171/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 171/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 171/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 172/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 172/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 172/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 172/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 172/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 172/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 172/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 172/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 172/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 172/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 173/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 173/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 2/5; 173/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 173/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 173/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 173/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 173/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 173/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 173/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 173/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 1/5; 174/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 174/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 174/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 174/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 174/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 174/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 174/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 174/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 5/5; 174/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 174/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 175/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 175/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 175/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 175/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 175/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 175/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 4/5; 175/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 175/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 175/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 175/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 176/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 176/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 176/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 176/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 3/5; 176/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 176/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 176/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 176/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 176/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 176/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 177/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 177/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 177/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 177/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 177/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 177/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 177/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 177/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 177/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 177/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 178/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 178/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 178/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 178/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 178/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 178/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 4/5; 178/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 178/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 178/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 178/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 179/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 179/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 179/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 179/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 179/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 179/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 4/5; 179/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 179/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 5/5; 179/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 179/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 180/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 180/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 180/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 180/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.3s\n",
      "[CV 3/5; 180/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 180/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 180/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 180/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 180/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 180/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 181/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 181/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 2/5; 181/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 181/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 181/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 181/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 181/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 181/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 5/5; 181/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 181/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.4s\n",
      "[CV 1/5; 182/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 182/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 182/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 182/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 182/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 182/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 182/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 182/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 182/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 182/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.3s\n",
      "[CV 1/5; 183/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 183/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 183/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 183/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 3/5; 183/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 183/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 183/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 183/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 5/5; 183/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 183/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 184/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 184/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 184/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 184/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 3/5; 184/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 184/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 4/5; 184/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 184/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 184/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 184/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 185/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 185/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.3s\n",
      "[CV 2/5; 185/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 185/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 185/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 185/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 185/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 185/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 185/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 185/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 186/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 186/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 2/5; 186/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 186/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 3/5; 186/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 186/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 4/5; 186/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 186/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 186/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 186/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 187/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 187/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 187/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 187/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 187/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 187/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 4/5; 187/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 187/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 5/5; 187/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 187/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 188/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 188/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 188/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 188/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 188/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 188/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 188/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 188/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 188/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 188/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 189/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 189/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.2s\n",
      "[CV 2/5; 189/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 189/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.3s\n",
      "[CV 3/5; 189/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 189/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 189/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 189/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 189/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 189/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.3s\n",
      "[CV 1/5; 190/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 190/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 2/5; 190/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 190/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 190/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 190/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 190/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 190/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 5/5; 190/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 190/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 191/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 191/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 2/5; 191/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 191/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 191/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 191/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 191/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 191/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 191/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 191/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 1/5; 192/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 192/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 2/5; 192/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 192/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 192/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 192/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 192/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 192/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 192/256] START activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 192/256] END activation_function=tanh, batch_size=10, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 1/5; 193/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 193/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 2/5; 193/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 193/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 193/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 193/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 193/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 193/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 5/5; 193/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 193/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 194/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 194/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 194/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 194/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 194/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 194/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 194/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 194/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 194/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 194/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 195/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 195/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 195/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 195/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 195/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 195/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 195/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 195/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 195/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 195/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 196/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 196/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 196/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 196/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 196/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 196/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 196/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 196/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 196/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 196/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 1/5; 197/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 197/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 197/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 197/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 197/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 197/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 197/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 197/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 5/5; 197/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 197/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 198/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 198/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 198/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 198/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 198/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 198/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 4/5; 198/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 198/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 198/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 198/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 199/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 199/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 199/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 199/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 199/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 199/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 199/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 199/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 199/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 199/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 200/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 200/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 200/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 200/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 200/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 200/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 200/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 200/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 200/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 200/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 1/5; 201/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 201/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 201/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 201/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 201/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 201/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 201/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 201/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 201/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 201/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 202/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 202/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 202/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 202/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 202/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 202/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 202/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 202/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 202/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 202/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 203/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 203/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 203/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 203/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 203/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 203/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 203/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 203/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 203/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 203/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 204/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 204/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 2/5; 204/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 204/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 3/5; 204/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 204/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 204/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 204/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 204/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 204/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 205/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 205/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 205/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 205/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 3/5; 205/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 205/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 205/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 205/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 205/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 205/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 1/5; 206/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 206/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 206/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 206/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 206/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 206/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 206/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 206/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 206/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 206/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 207/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 207/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 207/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 207/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 207/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 207/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 207/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 207/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 207/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 207/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 208/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 208/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 208/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 208/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 208/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 208/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 208/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 208/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 208/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 208/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 209/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 209/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 209/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 209/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 209/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 209/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 209/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 209/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 209/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 209/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 210/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 210/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 210/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 210/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 210/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 210/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 210/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 210/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 210/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 210/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 211/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 211/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 2/5; 211/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 211/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 211/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 211/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 211/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 211/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 211/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 211/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 212/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 212/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 212/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 212/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 212/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 212/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 212/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 212/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 212/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 212/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 213/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 213/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 213/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 213/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 213/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 213/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 213/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 213/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 213/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 213/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 214/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 214/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 214/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 214/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 214/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 214/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 214/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 214/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 214/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 214/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 215/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 215/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 215/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 215/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 215/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 215/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 215/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 215/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 215/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 215/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 216/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 216/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 2/5; 216/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 216/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 216/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 216/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 216/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 216/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 216/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 216/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 217/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 217/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 217/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 217/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 217/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 217/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 217/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 217/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 217/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 217/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 218/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 218/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 218/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 218/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 3/5; 218/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 218/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 218/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 218/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 218/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 218/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 219/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 219/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 219/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 219/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 219/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 219/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 219/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 219/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 219/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 219/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 220/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 220/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 220/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 220/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 220/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 220/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 220/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 220/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 220/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 220/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 221/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 221/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 2/5; 221/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 221/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 221/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 221/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 221/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 221/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 221/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 221/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 222/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 222/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.0s\n",
      "[CV 2/5; 222/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 222/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 222/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 222/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 222/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 222/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.1s\n",
      "[CV 5/5; 222/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 222/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 223/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 223/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 223/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 223/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 223/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 223/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 223/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 223/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 5/5; 223/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 223/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 1/5; 224/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 224/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 2/5; 224/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 224/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 3/5; 224/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 224/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 224/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 224/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 224/256] START activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 224/256] END activation_function=tanh, batch_size=20, dropout_rate=0.1, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 225/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 225/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 225/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 225/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 225/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 225/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 225/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 225/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 225/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 225/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 226/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 226/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 226/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 226/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 226/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 226/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 226/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 226/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 226/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 226/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 227/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 227/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 227/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 227/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.7s\n",
      "[CV 3/5; 227/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 227/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 227/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 227/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 227/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 227/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 228/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 228/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.6s\n",
      "[CV 2/5; 228/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 228/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 228/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 228/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 228/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 228/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 228/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 228/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 1/5; 229/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 229/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 229/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 229/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 229/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 229/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 229/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 229/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 229/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 229/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 230/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 230/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 230/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 230/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 230/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 230/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 230/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 230/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 5/5; 230/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 230/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 231/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 231/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 231/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 231/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 3/5; 231/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 231/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 231/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 231/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 231/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 231/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 232/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 232/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 232/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 232/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.7s\n",
      "[CV 3/5; 232/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 232/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 232/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 232/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 232/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 232/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 233/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 233/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.6s\n",
      "[CV 2/5; 233/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 233/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 233/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 233/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 233/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 233/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.5s\n",
      "[CV 5/5; 233/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 233/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 234/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 234/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 2/5; 234/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 234/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 234/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 234/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 4/5; 234/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 234/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.6s\n",
      "[CV 5/5; 234/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 234/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 235/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 235/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 235/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 235/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 235/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 235/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 4/5; 235/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 235/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 235/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 235/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 236/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 236/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 236/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 236/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 236/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 236/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 236/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 236/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 236/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 236/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 237/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 237/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 2/5; 237/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 237/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 3/5; 237/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 237/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.4s\n",
      "[CV 4/5; 237/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 237/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 237/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 237/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 1/5; 238/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 238/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 238/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 238/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 3/5; 238/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 238/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 4/5; 238/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 238/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 238/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 238/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.5s\n",
      "[CV 1/5; 239/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 239/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 2/5; 239/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 239/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 3/5; 239/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 239/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.6s\n",
      "[CV 4/5; 239/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 239/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 5/5; 239/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 239/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.4s\n",
      "[CV 1/5; 240/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 240/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 2/5; 240/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 240/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 240/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 240/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 240/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 240/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 5/5; 240/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 240/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.4s\n",
      "[CV 1/5; 241/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 241/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 241/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 241/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 241/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 241/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 241/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 241/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 241/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 241/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 242/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 242/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 242/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 242/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 242/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 242/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 4/5; 242/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 242/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 242/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 242/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 243/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 243/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 243/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 243/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 243/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 243/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 4/5; 243/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 243/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 243/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 243/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 1/5; 244/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 244/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 2/5; 244/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 244/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 3/5; 244/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 244/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 244/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 244/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 244/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 244/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 245/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 245/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 245/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 245/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 245/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 245/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 245/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 245/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 245/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 245/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 246/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 246/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 246/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 246/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.9s\n",
      "[CV 3/5; 246/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 246/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 4/5; 246/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 246/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 246/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 246/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 247/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 247/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 247/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 247/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 247/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 247/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 247/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 247/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 247/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 247/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 1/5; 248/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 248/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 2/5; 248/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 248/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 3/5; 248/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 248/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 4/5; 248/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 248/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 248/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 248/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 249/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 249/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 2/5; 249/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 249/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 249/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 249/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 4/5; 249/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 249/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 249/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 249/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 250/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 250/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 250/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 250/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 250/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 250/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 4/5; 250/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 250/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 250/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 250/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 251/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 251/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 2/5; 251/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 251/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 251/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 251/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 251/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 251/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 5/5; 251/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 251/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 1/5; 252/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 252/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 252/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 252/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 252/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 252/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 4/5; 252/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 252/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.0s\n",
      "[CV 5/5; 252/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 252/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 253/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 253/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 2/5; 253/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 253/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   0.8s\n",
      "[CV 3/5; 253/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 253/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 253/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 253/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 253/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 253/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.0s\n",
      "[CV 1/5; 254/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 254/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 2/5; 254/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 254/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 254/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 254/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.0s\n",
      "[CV 4/5; 254/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 254/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 254/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 254/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   0.8s\n",
      "[CV 1/5; 255/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 255/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 2/5; 255/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 255/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 255/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 255/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.0s\n",
      "[CV 4/5; 255/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 255/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   0.8s\n",
      "[CV 5/5; 255/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 255/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 256/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 256/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.9s\n",
      "[CV 2/5; 256/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 256/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 256/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 256/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 256/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 256/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n",
      "[CV 5/5; 256/256] START activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 256/256] END activation_function=tanh, batch_size=20, dropout_rate=0.2, epochs=50, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   0.8s\n"
     ]
    }
   ],
   "source": [
    "## Build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model1, param_grid = param_grids, cv = KFold(), verbose = 10)\n",
    "grid_result = grid.fit(x_std,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f315c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.9515870094299317, using {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7830097079277039,0.11521053976373634 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8431852221488952,0.07857891937758563 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8217886447906494,0.1078336184659992 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8296303153038025,0.0796219481241677 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7349327802658081,0.0759610046889625 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.736557137966156,0.129559888234479 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7363891005516052,0.1535027214323909 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7386109113693238,0.09315619640362674 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8122106075286866,0.0775074866818384 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8063293337821961,0.09938454461997859 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7733569860458374,0.14459989253334882 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8334764838218689,0.09210669900473 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7771844744682312,0.10841726024046809 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7753360748291016,0.07195503859399835 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.75005601644516,0.12866546085200717 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7365384817123413,0.12443864760867629 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9225728154182434,0.04181807049619678 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9263816356658936,0.03759267550945816 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9264749765396119,0.04173854938834545 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9012135863304138,0.05248333487335192 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7888349413871765,0.10869885335406688 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.83728529214859,0.08660957041625815 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7907953500747681,0.09571570062279176 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7946228504180908,0.11133770758895123 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9264002919197083,0.04270650053233921 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9246265769004822,0.04846044550788802 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8935586094856263,0.03617080155633251 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8973114371299744,0.052338768142594264 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7385922312736511,0.11863023518080944 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7966952919960022,0.06981385425533986 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7133495211601257,0.14098721675425466 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7635548949241638,0.13091406558496294 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7791075348854065,0.15250993481767408 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8005041122436524,0.10125469953199756 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.777352511882782,0.07875102815894035 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7907766938209534,0.11675135921489713 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7095033645629882,0.1610276218579903 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7018483877182007,0.09364983396726646 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7248132944107055,0.15405583424796385 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7326176285743713,0.13635299847664556 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7596900701522827,0.15057936712066963 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.775317394733429,0.0984111700855259 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7849327802658081,0.11727095240403479 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7734503388404846,0.08730484841141603 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7134241819381714,0.1186290810552322 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7328603506088257,0.08236295364533687 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.6864264249801636,0.08521652983123387 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7114451050758361,0.10715046292391979 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9264189720153808,0.037973381553045034 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.924533236026764,0.044943219889419954 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8935026168823242,0.0420900932794962 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8857356190681458,0.0563773201183478 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7850074768066406,0.07032536773447874 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.777091109752655,0.12409274179663976 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8062919974327087,0.10381019263515905 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7522591471672058,0.04509424402838392 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9070575118064881,0.04989003958896872 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9148618459701539,0.05148442809717347 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8992905139923095,0.052146905968536335 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9089619159698487,0.04276954640728494 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7463218927383423,0.12327909123481293 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8140216708183289,0.10014605064743035 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.6960418224334717,0.09735077825834258 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7542569041252136,0.06247439496962832 with: {'activation_function': 'softmax', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7364077687263488,0.15285944434968482 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7364077687263488,0.15285944434968482 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7364264488220215,0.14927184029909943 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.734522032737732,0.142381579736295 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7751493573188781,0.11999003651146478 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7558812499046326,0.13358867144421324 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7752053737640381,0.12191897878364548 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7463218808174134,0.08657138655826614 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.734522032737732,0.1426461412168627 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7345407009124756,0.13892299418301138 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7306198596954345,0.1471596559990732 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7325802803039551,0.1426535132113876 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7598581075668335,0.13978021718283407 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7906833410263061,0.12106566925232262 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7753547430038452,0.07873175729850583 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7500933527946472,0.09073674171257087 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9148244976997375,0.03977316762953288 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9206684112548829,0.04612521434804436 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8837752103805542,0.060342947656968564 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9244958877563476,0.047531480020928135 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8605302453041077,0.07089753954047309 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8528005957603455,0.07112753465205034 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7870425581932068,0.08862859608444064 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8469566941261292,0.07549709903862274 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8837751984596253,0.06398217135616109 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9342233061790466,0.04282037871788518 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9167662382125854,0.04347483027490798 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8954256892204284,0.053106904421562566 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8470126986503601,0.05983093545155943 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8819081306457519,0.04041416397797167 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8082710981369019,0.07238557804459217 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7813480257987976,0.05723125457226666 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.728659451007843,0.1510055827692834 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7402912616729737,0.15358536146170676 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7578043341636658,0.15155850433350757 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7946041822433472,0.10600260747992647 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7461164951324463,0.14089048197823736 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.74613516330719,0.14558913924979744 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.728659451007843,0.1510055827692834 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7383495211601258,0.1525569895928302 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7344660282135009,0.1532597362499129 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7364264488220215,0.14927184029909943 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.6614637851715088,0.11363672021094552 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7656833410263062,0.09859644671484548 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7732636332511902,0.11873640595326171 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7403846144676208,0.1327017057360463 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9032300233840942,0.04763130445447995 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8935026049613952,0.06055022987176905 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8799103856086731,0.05159039970493591 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9051157593727112,0.03284216340932976 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8121172666549683,0.09060625767396784 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8141523599624634,0.05749876443902521 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7617998480796814,0.086593198991074 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.80423823595047,0.09206029888045696 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8838498950004577,0.059557869570719395 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9167475700378418,0.043443214944303685 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8799477219581604,0.056778757949282097 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9109223127365113,0.045093410163613436 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8451082825660705,0.08953444078414742 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8276325583457946,0.08919166165505564 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.761818528175354,0.08624489761840376 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7888536095619202,0.0910424214131603 with: {'activation_function': 'softmax', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8819828271865845,0.028914816500266775 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8838872313499451,0.06952405508837622 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8896004438400269,0.05828125080363078 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8896751165390014,0.03703253072083793 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7830283761024475,0.09159986475105354 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7462471961975098,0.10934473600773849 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7387229204177856,0.04411927159622563 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7406833410263062,0.06074876769505706 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.908999252319336,0.047160092218081075 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.887658703327179,0.05829058108262894 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8897311449050903,0.0308321597037555 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8838125467300415,0.06503937872053846 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7737490773200989,0.04980455261959313 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8062920093536377,0.10224745559133226 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7889656543731689,0.07742360306428202 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7384241938591003,0.1363709058251401 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9496452569961548,0.03147826707980742 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9477408647537231,0.03266031123696497 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9302651166915894,0.038467793955604644 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9283607125282287,0.03746300095268491 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8160754203796386,0.047575610055290096 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8393577218055726,0.05100725123412916 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8160380959510803,0.07082861636280713 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8335138082504272,0.057129001537819114 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9437639951705933,0.03276310967480169 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9438760161399842,0.037851556190031034 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9322628736495971,0.041839332642607835 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9341112732887268,0.03440147100304281 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8314413905143738,0.08249078402561819 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8353621959686279,0.06555224156153425 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8199589252471924,0.048805521920434615 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8181105375289917,0.05345263843965639 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.895500373840332,0.053621700314418395 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9012509465217591,0.04675700518097448 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8780619859695434,0.05878314329769485 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8993465185165406,0.056788021333276775 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7269230842590332,0.12355188738171899 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7733569860458374,0.11223620127518855 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.69021657705307,0.09280213141124725 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.761762511730194,0.0968822301467733 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8896377921104431,0.059533335536738265 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9090552568435669,0.05365986927861737 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.889693808555603,0.05738006430295147 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8877147197723388,0.060320656148221234 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7113890886306763,0.1141637264941389 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7349514603614807,0.023234812456638713 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7307879090309143,0.1193268476544266 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7385922312736511,0.11763609222487116 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9341299414634705,0.03543892690542465 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9360716938972473,0.034459684392436095 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9380321145057678,0.03803942973775875 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9418782711029052,0.030040346329832098 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8005974531173706,0.06173776188164714 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8158887267112732,0.09183131530160094 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8004480957984924,0.10435637101848153 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7733943343162537,0.11653068240461532 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9303211212158203,0.04202952994859581 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.940011203289032,0.036273159253105515 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9323002219200134,0.04387873292753378 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9515870094299317,0.029362918120393742 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8160194158554077,0.09438471105191897 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8643577337265015,0.06943317756821586 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7811239719390869,0.09138344543291416 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8083271026611328,0.0751742962512905 with: {'activation_function': 'tanh', 'batch_size': 10, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8761015653610229,0.0661421438626511 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8837938904762268,0.05870184569854115 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8741224765777588,0.06645958614737364 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9012696027755738,0.05192755616917159 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7695295095443726,0.07422587619097157 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8161314487457275,0.056611022714769935 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.759783411026001,0.10195363686111501 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8063480138778687,0.08268073269800047 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8605862617492676,0.0709911051159723 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8838312149047851,0.052314124881515316 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8781180024147034,0.04556180231438485 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8955750584602356,0.04802675256928497 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7832337737083435,0.05941794598020577 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8316654324531555,0.052307127260658955 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.754182231426239,0.061071882661864405 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7810866236686707,0.10314390410877412 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9399365186691284,0.030914648342667026 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9419156193733216,0.03234240683145936 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9224981307983399,0.04792477492472467 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9302651405334472,0.03486894812512342 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8838125467300415,0.0595940584617224 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8994585394859314,0.02466067445784974 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8587565302848816,0.060215605668309774 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8722554206848144,0.0704285343854073 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9438013434410095,0.03154356477924709 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9399738669395447,0.03200096213505861 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9341859579086303,0.03630612794317599 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9515496611595153,0.026765796542085205 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.895500385761261,0.06076002607325597 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8644697308540344,0.06137404445541921 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8683719158172607,0.050716018573362366 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.860511577129364,0.07867929140282606 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8683905959129333,0.06242182382044586 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8837938904762268,0.05870184569854115 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8780993461608887,0.06719136666854338 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8877333760261535,0.055808253562307814 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7384428739547729,0.1243406458174391 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8143017172813416,0.03873975395883535 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7173263549804687,0.10204188784191749 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.7733943223953247,0.10068258460515596 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8701643109321594,0.07608243332430281 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.868409276008606,0.06964935353475102 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8857542872428894,0.062184659745131314 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8819081425666809,0.062066414587974225 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.7596340417861939,0.14797794397490713 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.7617438316345215,0.09712927969502681 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.7752800703048706,0.11046708469666186 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8296489834785461,0.08229947651059438 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 10, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9360903739929199,0.0338529003414727 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9438200116157531,0.03544477293076307 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9206497311592102,0.04496580689664828 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.9457430958747863,0.02917681476493909 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8586071729660034,0.07102908790932026 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8625466823577881,0.04700681617461545 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8277072429656982,0.07068603260813298 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8412248015403747,0.08257963642210571 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'uniform', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n",
      "0.9419342756271363,0.036646643147787025 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}\n",
      "0.9418782591819763,0.036292778364035924 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 4}\n",
      "0.9322628736495971,0.03855648161357685 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 2}\n",
      "0.951568341255188,0.027421740185401212 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 8, 'neuron2': 4}\n",
      "0.8375653386116028,0.02997490661601725 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 2}\n",
      "0.8760642409324646,0.06307301965012234 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 4, 'neuron2': 4}\n",
      "0.8237677335739135,0.08835520202280317 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 2}\n",
      "0.8606422662734985,0.04804562062562573 with: {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.2, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.1, 'neuron1': 8, 'neuron2': 4}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25abf97",
   "metadata": {},
   "source": [
    "###  Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d117f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "52/52 [==============================] - 0s 704us/step - loss: 0.6692 - mse: 0.6692\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 0s 677us/step - loss: 0.5404 - mse: 0.5404\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.4412 - mse: 0.4412\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 0s 763us/step - loss: 0.3630 - mse: 0.3630\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.3017 - mse: 0.3017\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.2554 - mse: 0.2554\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.2330 - mse: 0.2330\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.2221 - mse: 0.2221\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 0s 658us/step - loss: 0.2013 - mse: 0.2013\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.2136 - mse: 0.2136\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 0s 633us/step - loss: 0.1924 - mse: 0.1924\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.1958 - mse: 0.1958\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 0s 743us/step - loss: 0.1983 - mse: 0.1983\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 0s 700us/step - loss: 0.2016 - mse: 0.2016\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 0s 743us/step - loss: 0.1849 - mse: 0.1849\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.1765 - mse: 0.1765\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1818 - mse: 0.1818\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.1750 - mse: 0.1750\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1611 - mse: 0.1611\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 0s 721us/step - loss: 0.1618 - mse: 0.1618\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1651 - mse: 0.1651\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.1545 - mse: 0.1545\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.1495 - mse: 0.1495\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.1505 - mse: 0.1505\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1393 - mse: 0.1393\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.1309 - mse: 0.1309\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.1313 - mse: 0.1313\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.1161 - mse: 0.1161\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 0s 939us/step - loss: 0.1212 - mse: 0.1212\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.1287 - mse: 0.1287\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 0s 724us/step - loss: 0.1080 - mse: 0.1080\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.1118 - mse: 0.1118\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 0s 958us/step - loss: 0.1068 - mse: 0.1068\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.1010 - mse: 0.1010\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.0981 - mse: 0.0981\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.0972 - mse: 0.0972\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 0s 742us/step - loss: 0.0862 - mse: 0.0862\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.0731 - mse: 0.0731\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.0791 - mse: 0.0791\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0804 - mse: 0.0804\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 0s 723us/step - loss: 0.0674 - mse: 0.0674\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.0756 - mse: 0.0756\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.0648 - mse: 0.0648\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 0s 936us/step - loss: 0.0660 - mse: 0.0660\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.0626 - mse: 0.0626\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.0613 - mse: 0.0613\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.0592 - mse: 0.0592\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0617 - mse: 0.0617\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.0566 - mse: 0.0566\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 0s 919us/step - loss: 0.0540 - mse: 0.0540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a6a755250>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "final_model = Sequential()\n",
    "final_model.add(Dense(8, input_dim=28, kernel_initializer ='normal', activation='tanh'))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(Dense(4, kernel_initializer='uniform', activation='softmax'))\n",
    "final_model.add(Dropout(0.2))\n",
    "final_model.add(Dense(1, kernel_initializer='uniform', activation='linear'))\n",
    "\n",
    "adam=adam_v2.Adam(learning_rate = 0.01)\n",
    "## compile model\n",
    "final_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "## fit the model \n",
    "final_model.fit(x_std,y, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f85d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 624us/step - loss: 0.5956 - mse: 0.5956\n",
      "mse\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "score = final_model.evaluate(x,y)\n",
    "print((final_model.metrics_names[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa7538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
